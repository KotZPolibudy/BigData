{"cells": [{"cell_type": "markdown", "id": "861b6d4a-0de6-42ba-97a5-beef1f82f292", "metadata": {}, "source": "# Projekt Apache Spark"}, {"cell_type": "markdown", "id": "7b301ae8-ceff-4dbf-8d04-75bb4eb52480", "metadata": {}, "source": "# Wprowadzenie\n\nWykorzystuj\u0105c ten notatnik jako szablon zrealizuj projekt Apache Spark zgodnie z przydzielonym zestawem. \n\nKilka uwag:\n\n* Nie modyfikuj ani nie usuwaj paragraf\u00f3w *markdown* w tym notatniku, chyba \u017ce wynika to jednoznacznie z instrukcji. \n* Istniej\u0105ce paragrafy zawieraj\u0105ce *kod* uzupe\u0142nij w razie potrzeby zgodnie z instrukcjami\n    - nie usuwaj ich\n    - nie usuwaj zawartych w nich instrukcji oraz kodu\n    - nie modyfikuj ich, je\u015bli instrukcje jawnie tego nie nakazuj\u0105\n* Mo\u017cesz dodawa\u0107 nowe paragrafy zar\u00f3wno zawieraj\u0105ce kod jak i komentarze dotycz\u0105ce tego kodu (markdown)"}, {"cell_type": "markdown", "id": "e69d12f1-1013-4c74-b6aa-686ccfcbdd5c", "metadata": {}, "source": "# Tre\u015b\u0107 projektu\n\nPoni\u017cej w paragrafie markdown wstaw tytu\u0142 przydzielonego zestawu"}, {"cell_type": "markdown", "id": "adfc4ff6-4d43-49ed-a0d1-8b6988eaec16", "metadata": {}, "source": "# Zestaw 3 \u2013 nyc-accidents\n\n**Uwaga**\n\n- W ramach wzorca nie s\u0105 spe\u0142nione \u017cadne regu\u0142y projektu. \n- Brak konsekwencji w wykorzystaniu w\u0142a\u015bciwego API w ramach poszczeg\u00f3lnych cz\u0119\u015bci\n- Zadanie *misji g\u0142\u00f3wnej* polega na zliczeniu s\u0142\u00f3wek.  "}, {"cell_type": "markdown", "id": "5e128e43-6cce-4ffa-9609-9fae4b164ae9", "metadata": {}, "source": "# Dzia\u0142ania wst\u0119pne \n\nUruchom poni\u017cszy paragraf, aby utworzy\u0107 obiekty kontekstu Sparka. Je\u015bli jest taka potrzeba dostosuj te polecenia. Pami\u0119taj o potrzebnych bibliotekach."}, {"cell_type": "code", "execution_count": 1, "id": "26fb1050-386f-4398-ba5a-b45f5065d87b", "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.sql import SparkSession\n\n# Spark session & context\nspark = SparkSession.builder.getOrCreate()\n\nsc = spark.sparkContext"}, {"cell_type": "markdown", "id": "8695a354-52bc-4bba-8222-7121bf07ae90", "metadata": {}, "source": "W poni\u017cszym paragrafie uzupe\u0142nij polecenia definiuj\u0105ce poszczeg\u00f3lne zmienne. \n\nPami\u0119taj aby\u015b:\n\n* w p\u00f3\u017aniejszym kodzie, dla wszystkich cze\u015bci projektu, korzysta\u0142 z tych zdefiniowanych zmiennych. Wykorzystuj je analogicznie jak parametry\n* przed ostateczn\u0105 rejestracj\u0105 projektu usun\u0105\u0142 ich warto\u015bci, tak aby nie pozostawia\u0107 w notatniku niczego co mog\u0142oby identyfikowa\u0107 Ciebie jako jego autora"}, {"cell_type": "code", "execution_count": 2, "id": "e883af01-7117-4faa-a840-7ff807a195d9", "metadata": {"tags": []}, "outputs": [], "source": "# pe\u0142na \u015bcie\u017cka do katalogu w zasobniku zawieraj\u0105cego podkatalogi `datasource1` i `datasource4` \n# z danymi \u017ar\u00f3d\u0142owymi\ninput_dir = \"/input\""}, {"cell_type": "markdown", "id": "4601cc7a-3ed5-47e2-994f-ebec642049b5", "metadata": {}, "source": "Nie modyfikuj poni\u017cszych paragraf\u00f3w. Wykonaj je i u\u017cywaj zdefniowanych poni\u017cej zmiennych jak parametr\u00f3w Twojego programu."}, {"cell_type": "code", "execution_count": 3, "id": "6167e297-01ed-463e-bb81-9104d7cf7093", "metadata": {"tags": []}, "outputs": [], "source": "# NIE ZMIENIA\u0106\n# \u015bcie\u017cki dla danych \u017ar\u00f3d\u0142owych \ndatasource1_dir = input_dir + \"/datasource1\"\ndatasource4_dir = input_dir + \"/datasource4\"\n\n# nazwy i \u015bcie\u017cki dla wynik\u00f3w dla misji g\u0142\u00f3wnej \n# cz\u0119\u015b\u0107 1 (Spark Core - RDD) \nrdd_result_dir = \"/tmp/output1\"\n\n# cz\u0119\u015b\u0107 2 (Spark SQL - DataFrame)\ndf_result_table = \"output2\"\n\n# cz\u0119\u015b\u0107 3 (Pandas API on Spark)\nps_result_file = \"/tmp/output3.json\""}, {"cell_type": "code", "execution_count": 4, "id": "e36e0314-a4ac-4096-9e4b-23fd4a73e0a9", "metadata": {"tags": []}, "outputs": [], "source": "# NIE ZMIENIA\u0106\nimport os\ndef remove_file(file):\n    if os.path.exists(file):\n        os.remove(file)\n\nremove_file(\"metric_functions.py\")\nremove_file(\"tools_functions.py\")"}, {"cell_type": "code", "execution_count": 5, "id": "1b4b8e00-10ae-47dc-b623-d1dacbe9c86b", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "3322"}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": "# NIE ZMIENIA\u0106\nimport requests\nr = requests.get(\"https://jankiewicz.pl/bigdata/metric_functions.py\", allow_redirects=True)\nopen('metric_functions.py', 'wb').write(r.content)\nr = requests.get(\"https://jankiewicz.pl/bigdata/tools_functions.py\", allow_redirects=True)\nopen('tools_functions.py', 'wb').write(r.content)"}, {"cell_type": "code", "execution_count": 6, "id": "0a433894-dc97-46f2-be51-9f40fa36894f", "metadata": {"tags": []}, "outputs": [], "source": "# NIE ZMIENIA\u0106\n%run metric_functions.py\n%run tools_functions.py"}, {"cell_type": "markdown", "id": "c9d3a9dc-ac3b-4316-abb9-365caa1d7185", "metadata": {}, "source": "Poni\u017csze paragrafy maj\u0105 na celu usun\u0105\u0107 ewentualne pozosta\u0142o\u015bci poprzednich uruchomie\u0144 tego lub innych notatnik\u00f3w"}, {"cell_type": "code", "execution_count": 7, "id": "08091c72-937f-41c2-9afe-d1505862bf1c", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "rm: `/tmp/output1': No such file or directory\n"}, {"name": "stdout", "output_type": "stream", "text": "Error deleting file /tmp/output1: Command '['hadoop', 'fs', '-rm', '-r', '/tmp/output1']' returned non-zero exit status 1.\n"}], "source": "# NIE ZMIENIA\u0106\n# usuni\u0119cie miejsca docelowego dla cz\u0119\u015b\u0107 1 (Spark Core - RDD) \ndelete_dir(spark, rdd_result_dir)"}, {"cell_type": "code", "execution_count": 8, "id": "f3e863c0-c824-47bd-b53a-ce3b1fd6d453", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n"}, {"name": "stdout", "output_type": "stream", "text": "The table output2 does not exist.\nError deleting file file:/spark-warehouse/output2: Command '['hadoop', 'fs', '-rm', '-r', 'file:/spark-warehouse/output2']' returned non-zero exit status 1.\n"}, {"name": "stderr", "output_type": "stream", "text": "rm: `file:/spark-warehouse/output2': No such file or directory\n"}], "source": "# NIE ZMIENIA\u0106\n# usuni\u0119cie miejsca docelowego dla cz\u0119\u015b\u0107 2 (Spark SQL - DataFrame) \ndrop_table(spark, df_result_table)"}, {"cell_type": "code", "execution_count": 9, "id": "72956a1a-da48-4d2b-a07a-e03d56431d6e", "metadata": {"tags": []}, "outputs": [], "source": "# NIE ZMIENIA\u0106\n# usuni\u0119cie miejsca docelowego dla cz\u0119\u015b\u0107 3 (Pandas API on Spark) \nremove_file(ps_result_file)"}, {"cell_type": "code", "execution_count": 10, "id": "b9e423d4-92b8-4161-98da-1a867f86d780", "metadata": {"tags": []}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://pbd-cluster-m.europe-west4-a.c.big-data-2024-10-wk.internal:33051\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.5.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7fe81aed96d0>"}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": "# NIE ZMIENIA\u0106\nspark"}, {"cell_type": "markdown", "id": "14faf05b-6c52-4b02-b2e5-2ddb3f38c704", "metadata": {}, "source": "***Uwaga!***\n\nUruchom poni\u017cszy paragraf i sprawd\u017a czy adres, pod kt\u00f3rym dost\u0119pny *Apache Spark Application UI* jest poprawny wywo\u0142uj\u0105c nast\u0119pny testowy paragraf. \n\nW razie potrzeby okre\u015bl samodzielnie poprawny adres, pod kt\u00f3rym dost\u0119pny *Apache Spark Application UI*"}, {"cell_type": "code", "execution_count": 11, "id": "32acf3d2-ec4e-469d-bb0b-5f260c2c8e3b", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "'http://pbd-cluster-m.europe-west4-a.c.big-data-2024-10-wk.internal:33051'"}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": "# adres URL, pod kt\u00f3rym dost\u0119pny Apache Spark Application UI (REST API)\n# \nspark_ui_address = extract_host_and_port(spark, \"http://localhost:4041\")\nspark_ui_address"}, {"cell_type": "code", "execution_count": 12, "id": "32c2329e-1d7a-465f-a23b-333f95bf7deb", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "{'numTasks': 0,\n 'numActiveTasks': 0,\n 'numCompleteTasks': 0,\n 'numFailedTasks': 0,\n 'numKilledTasks': 0,\n 'numCompletedIndices': 0,\n 'executorDeserializeTime': 0,\n 'executorDeserializeCpuTime': 0,\n 'executorRunTime': 0,\n 'executorCpuTime': 0,\n 'resultSize': 0,\n 'jvmGcTime': 0,\n 'resultSerializationTime': 0,\n 'memoryBytesSpilled': 0,\n 'diskBytesSpilled': 0,\n 'peakExecutionMemory': 0,\n 'inputBytes': 0,\n 'inputRecords': 0,\n 'outputBytes': 0,\n 'outputRecords': 0,\n 'shuffleRemoteBlocksFetched': 0,\n 'shuffleLocalBlocksFetched': 0,\n 'shuffleFetchWaitTime': 0,\n 'shuffleRemoteBytesRead': 0,\n 'shuffleRemoteBytesReadToDisk': 0,\n 'shuffleLocalBytesRead': 0,\n 'shuffleReadBytes': 0,\n 'shuffleReadRecords': 0,\n 'shuffleWriteBytes': 0,\n 'shuffleWriteTime': 0,\n 'shuffleWriteRecords': 0}"}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": "# testowy paragraf\ntest_metrics = get_current_metrics(spark_ui_address)\ntest_metrics"}, {"cell_type": "markdown", "id": "f5ccca69-c577-440c-aa5c-c9df3a54e127", "metadata": {}, "source": "# Cz\u0119\u015b\u0107 1 - Spark Core (RDD)\n\n## Misje poboczne\n\nW ponizszych paragrafach wprowad\u017a swoje rozwi\u0105zania *misji pobocznych*, o ile **nie** chcesz, aby oceniana by\u0142a *misja g\u0142\u00f3wna*. W przeciwnym przypadku **KONIECZNIE** pozostaw je **puste**.  "}, {"cell_type": "code", "execution_count": null, "id": "f0af3440-983a-4cac-a8e7-4908b010947c", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "5fc37879-e0fa-4c4a-bd0d-4c01c3ecf38a", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "d303a72b-4083-470e-b25d-3224360ee94f", "metadata": {}, "source": "## Misja g\u0142\u00f3wna \n\nPoni\u017cszy paragraf zapisuje metryki przed uruchomieniem Twojego rozwi\u0105zania *misji g\u0142\u00f3wnej*. \n\nNie musisz go uruchamia\u0107 podczas implementacji rozwi\u0105zania."}, {"cell_type": "code", "execution_count": 13, "id": "037689d7-f0ee-4165-bef0-83fa7f3e8346", "metadata": {"tags": []}, "outputs": [], "source": "# NIE ZMIENIA\u0106\nbefore_rdd_metrics = get_current_metrics(spark_ui_address)"}, {"cell_type": "markdown", "id": "b23971c0-cec7-4ea8-befb-7f063dce863c", "metadata": {}, "source": "W poni\u017cszych paragrafach wprowad\u017a **rozwi\u0105zanie** *misji g\u0142\u00f3wnej* oparte na *RDD API*. \n\nPami\u0119taj o wydajno\u015bci Twojego przetwarzania, *RDD API* tego wymaga. \n\nNie wprowadzaj w poni\u017cszych paragrafach \u017cadnego kodu, w przypadku wykorzystania *misji pobocznych*."}, {"cell_type": "code", "execution_count": 14, "id": "8af00c41-02a9-4a85-b3c6-bc41098edbe2", "metadata": {"tags": []}, "outputs": [], "source": "from datetime import datetime\n\n\ndef safe_float(value):\n    try:\n        return float(value)\n    except (ValueError, TypeError):\n        return 0.0\n\n\ndef map_by_person_type(attributes: list, street: str, borough: str, person_type: str):\n    if person_type == \"pedestrian\":\n        injured = safe_float(attributes[11])\n        killed = safe_float(attributes[12])\n        if injured > 0 or killed > 0:\n            return street, borough, \"pedestrian\", injured, killed\n    elif person_type == \"cyclist\":\n        injured = safe_float(attributes[13])\n        killed = safe_float(attributes[14])\n        if injured > 0 or killed > 0:\n            return street, borough, \"cyclist\", injured, killed\n    elif person_type == \"motorist\":\n        injured = safe_float(attributes[15])\n        killed = safe_float(attributes[16])\n        if injured > 0 or killed > 0:\n            return street, borough, \"motorist\", injured, killed\n    return None\n\n\ndef extract_street_data(line):\n    attributes = line.split(\",\")\n    streets = [attributes[i] for i in range(6, 9) if attributes[i] != \"\"]\n    return [\n        result for street in streets\n        for result in filter(None, [\n            map_by_person_type(attributes, street, attributes[28], \"cyclist\"),\n            map_by_person_type(attributes, street, attributes[28], \"motorist\"),\n            map_by_person_type(attributes, street, attributes[28], \"pedestrian\")\n        ])\n    ]\n\n\ndef filter_date(line):\n    try:\n        fields = line.split(\",\")\n        date = fields[0]\n        if date == \"\":\n            return False\n        date_obj = datetime.strptime(date, \"%m/%d/%Y\")\n        return date_obj.year > 2012\n    except Exception:\n        return False\n\ndef map_add_borough(line):\n    fields = line.split(\",\")\n    zip_code = fields[2]\n    borough = broadcast_zip_to_borough.value.get(zip_code)\n    if borough:\n        return line + f\",{borough}\"\n    return None\n\n"}, {"cell_type": "code", "execution_count": 15, "id": "e85d1fab-5ed0-4940-a4f4-e21ee963c6ee", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Wczytanie plik\u00f3w\naccidents = sc.textFile(datasource1_dir)\nzip_to_borough = sc.textFile(datasource4_dir) \\\n    .map(lambda line: line.split(\",\")) \\\n    .filter(lambda x: len(x) == 2 and x[0] != \"ZIP CODE\") \\\n    .map(lambda x: (x[0].strip(), x[1].strip())) \\\n    .collectAsMap()\nbroadcast_zip_to_borough = sc.broadcast(zip_to_borough)"}, {"cell_type": "code", "execution_count": 16, "id": "008c6269-7653-41d3-a925-b00e8e814fa8", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Filtrowanie danych\nfiltered_with_borough = (\n    accidents.filter(filter_date)\n             .map(map_add_borough)\n             .filter(lambda x: x is not None)\n)\n\n# Agregacja wed\u0142ug dzielnic\nborough_data = (\n    filtered_with_borough\n    .map(lambda line: (line.split(\",\")[28], 1))\n    .filter(lambda x: x[0] != '')\n    .reduceByKey(lambda a, b: a + b)\n)\n\ntotal_accidents, borough_count = borough_data.aggregate(\n    (0, 0),\n    lambda acc, value: (acc[0] + value[1], acc[1] + 1),\n    lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n)\naverage_accidents = total_accidents / borough_count\n\nborough_above_average = (\n    borough_data.filter(lambda x: x[1] > average_accidents)\n                .map(lambda x: x[0])\n                .collect()\n)\n\n# Broadcast powy\u017cej \u015bredniej\nborough_above_average_broadcast = sc.broadcast(borough_above_average)\n\n# Dane dla ulic\nstreet_data = (\n    filtered_with_borough\n    .filter(lambda line: line.split(\",\")[28] in borough_above_average_broadcast.value)\n    .flatMap(extract_street_data)\n)\n\ngrouped_data = (\n    street_data.map(lambda x: ((x[0], x[1], x[2]), (x[3], x[4])))\n               .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n)\n\nfinal_rdd = (\n    grouped_data.map(lambda x: (\n        x[0][2],\n        (x[0][0], x[0][1], x[1][0], x[1][1], x[1][0] + x[1][1])\n    ))\n    .combineByKey(\n        lambda value: [(value[4], value)],\n        lambda acc, value: sorted(acc + [(value[4], value)], reverse=True)[:3],\n        lambda acc1, acc2: sorted(acc1 + acc2, reverse=True)[:3]\n    )\n    .mapValues(lambda values: [v[1] for v in values])\n    .flatMapValues(\n        lambda values: sorted(values, key=lambda s: s[4], reverse=True)[:3]\n    )\n    .map(lambda x: (x[1][0], x[1][1], x[0], x[1][2], x[1][3]))\n)\n"}, {"cell_type": "code", "execution_count": 17, "id": "91d77fd7-1f15-4365-ae80-c902aeb55ce7", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Zapis wyniku do pliku pickle\nfinal_rdd.saveAsPickleFile(rdd_result_dir)"}, {"cell_type": "markdown", "id": "42d8b5ec-b799-4177-8e4a-80a583d995e7", "metadata": {}, "source": "Poni\u017cszy paragraf zapisuje metryki po uruchomieniu Twojego rozwi\u0105zania *misji g\u0142\u00f3wnej*. \n\nNie musisz go uruchamia\u0107 podczas implementacji rozwi\u0105zania."}, {"cell_type": "code", "execution_count": 18, "id": "4325d378-b145-4e8f-8d37-80a072b506c3", "metadata": {}, "outputs": [], "source": "# NIE ZMIENIA\u0106\nafter_rdd_metrics = get_current_metrics(spark_ui_address)"}, {"cell_type": "code", "execution_count": 19, "id": "7502f8e6-34c9-4339-ae8f-90a5f8870051", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 10:====================================================>  (95 + 4) / 100]\r"}, {"name": "stdout", "output_type": "stream", "text": "('ATLANTIC AVENUE', 'BROOKLYN', 'motorist', 2861.0, 5.0)\n('LINDEN BOULEVARD', 'BROOKLYN', 'motorist', 2409.0, 5.0)\n('NORTHERN BOULEVARD', 'QUEENS', 'motorist', 1935.0, 4.0)\n('BROADWAY', 'MANHATTAN', 'pedestrian', 1050.0, 9.0)\n('3 AVENUE', 'MANHATTAN', 'pedestrian', 858.0, 11.0)\n('2 AVENUE', 'MANHATTAN', 'pedestrian', 846.0, 7.0)\n('2 AVENUE', 'MANHATTAN', 'cyclist', 479.0, 0.0)\n('BROADWAY', 'MANHATTAN', 'cyclist', 468.0, 1.0)\n('1 AVENUE', 'MANHATTAN', 'cyclist', 416.0, 1.0)\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# for row in final_rdd.collect():\n#     print(row)"}, {"cell_type": "markdown", "id": "28137d3d-6f0d-443f-97b8-38104aaced6d", "metadata": {}, "source": "# Cz\u0119\u015b\u0107 2 - Spark SQL (DataFrame)\n\n## Misje poboczne\n\nW ponizszych paragrafach wprowad\u017a swoje rozwi\u0105zania *misji pobocznych*, o ile **nie** chcesz, aby oceniana by\u0142a *misja g\u0142\u00f3wna*. W przeciwnym przypadku **KONIECZNIE** pozostaw je **puste**.  "}, {"cell_type": "code", "execution_count": null, "id": "6d045dae-5826-4015-8833-564d356db1f8", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "f7738406-c426-4238-b0fb-983f4585bc5a", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "5e7e569f-5f6b-4a98-b177-1b6fb0fc3333", "metadata": {}, "source": "## Misja g\u0142\u00f3wna \n\nPoni\u017cszy paragraf zapisuje metryki przed uruchomieniem Twojego rozwi\u0105zania *misji g\u0142\u00f3wnej*. \n\nNie musisz go uruchamia\u0107 podczas implementacji rozwi\u0105zania."}, {"cell_type": "code", "execution_count": 20, "id": "6329c04b-3e50-41a8-93f1-333ac0ea64ce", "metadata": {}, "outputs": [], "source": "# NIE ZMIENIA\u0106\nbefore_df_metrics = get_current_metrics(spark_ui_address)"}, {"cell_type": "markdown", "id": "4c2cfb0d-51b6-45bb-b173-ab8ac630d4f3", "metadata": {}, "source": "W poni\u017cszych paragrafach wprowad\u017a **rozwi\u0105zanie** *misji g\u0142\u00f3wnej* swojego projektu oparte o *DataFrame API*. \n\nPami\u0119taj o wydajno\u015bci Twojego przetwarzania, *DataFrame API* nie jest w stanie wszystkiego \"naprawi\u0107\". \n\nNie wprowadzaj w poni\u017cszych paragrafach \u017cadnego kodu, w przypadku wykorzystania *misji pobocznych*."}, {"cell_type": "code", "execution_count": 21, "id": "eca6e627-0ce5-4c48-b441-3bcc14e32f36", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, to_date, year, avg, stack, sum as _sum, row_number, desc, broadcast, lit\nfrom pyspark.sql.window import Window\n\n# Wczytanie danych\naccidents_df = spark.read.csv(datasource1_dir, header=False, inferSchema=True).toDF(\n    \"crash_date\", \"crash_time\", \"zip_code\", \"latitude\", \"longitude\", \"location\",\n    \"on_street_name\", \"cross_street_name\", \"off_street_name\", \"number_of_persons_injured\",\n    \"number_of_persons_killed\", \"number_of_pedestrians_injured\", \"number_of_pedestrians_killed\",\n    \"number_of_cyclist_injured\", \"number_of_cyclist_killed\", \"number_of_motorist_injured\",\n    \"number_of_motorist_killed\", \"contributing_factor_vehicle_1\", \"contributing_factor_vehicle_2\",\n    \"contributing_factor_vehicle_3\", \"contributing_factor_vehicle_4\", \"contributing_factor_vehicle_5\",\n    \"collision_id\", \"vehicle_type_code1\", \"vehicle_type_code2\", \"vehicle_type_code3\",\n    \"vehicle_type_code4\", \"vehicle_type_code5\"\n)\n\nzip_borough_df = spark.read.csv(datasource4_dir, header=True, inferSchema=True).withColumnRenamed(\"ZIP CODE\", \"zip_code\").withColumnRenamed(\"BOROUGH\", \"borough\")"}, {"cell_type": "code", "execution_count": 22, "id": "bcc4aaa9-8dc2-4726-871e-5e2450ba3fa8", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "filtered_accidents_df = (\n    accidents_df\n    .filter(\n        (col(\"zip_code\").isNotNull()) & \n        (col(\"crash_date\").isNotNull()) & \n        (year(to_date(col(\"crash_date\"), \"MM/dd/yyyy\")) > 2012)\n    )\n)\n\n# odfiltrowanie tylko potrzebnych kolumn przed join\nfiltered_accidents_df = filtered_accidents_df.select(\n    \"zip_code\", \"crash_date\", \"on_street_name\", \"cross_street_name\", \"off_street_name\",\n    \"number_of_pedestrians_injured\", \"number_of_pedestrians_killed\",\n    \"number_of_cyclist_injured\", \"number_of_cyclist_killed\",\n    \"number_of_motorist_injured\", \"number_of_motorist_killed\"\n)\n\naccidents_with_borough_df = filtered_accidents_df.join(\n    broadcast(zip_borough_df),\n    \"zip_code\",\n    \"inner\"\n)\n\n\nborough_data_df = (\n    accidents_with_borough_df\n    .groupBy(\"borough\")\n    .count()\n    .withColumnRenamed(\"count\", \"total_count\")\n)\n\naverage_count = borough_data_df.select(avg(\"total_count\")).first()[0]\n\nborough_above_average_df = borough_data_df.filter(col(\"total_count\") > lit(average_count))\n\nstreet_data_df = (\n    accidents_with_borough_df\n    .selectExpr(\n        \"borough\",\n        \"stack(3, on_street_name, cross_street_name, off_street_name) as street_name\",\n        \"number_of_pedestrians_injured\", \"number_of_pedestrians_killed\",\n        \"number_of_cyclist_injured\", \"number_of_cyclist_killed\",\n        \"number_of_motorist_injured\", \"number_of_motorist_killed\"\n    )\n    .filter(col(\"street_name\").isNotNull())\n    .join(borough_above_average_df.select(\"borough\"), on=\"borough\", how=\"inner\")\n)\n\n\nall_streets_df = (\n    street_data_df\n    .select(\n        \"street_name\", \"borough\",\n        \"number_of_pedestrians_injured\", \"number_of_pedestrians_killed\",\n        \"number_of_cyclist_injured\", \"number_of_cyclist_killed\",\n        \"number_of_motorist_injured\", \"number_of_motorist_killed\"\n    )\n    .selectExpr(\n        \"street_name\", \"borough\",\n        \"stack(3, \" +\n        \"'pedestrian', number_of_pedestrians_injured, number_of_pedestrians_killed, \" +\n        \"'cyclist', number_of_cyclist_injured, number_of_cyclist_killed, \" +\n        \"'motorist', number_of_motorist_injured, number_of_motorist_killed) \" +\n        \"as (person_type, injured, killed)\"\n    )\n    .filter((col(\"injured\") > 0) | (col(\"killed\") > 0))\n)\n\n\ngrouped_streets_df = (\n    all_streets_df\n    .groupBy(\"street_name\", \"borough\", \"person_type\")\n    .agg(\n        _sum(\"injured\").alias(\"total_injured\"),\n        _sum(\"killed\").alias(\"total_killed\")\n    )\n)\n\nwindow_spec = Window.partitionBy(\"person_type\").orderBy(desc(\"total_injured\"), desc(\"total_killed\"))\n\nfinal_df = (\n    grouped_streets_df\n    .withColumn(\"rank\", row_number().over(window_spec))\n    .filter(col(\"rank\") <= 3)\n    .select(\"street_name\", \"borough\", \"person_type\", \"total_injured\", \"total_killed\")\n)\n\n"}, {"cell_type": "code", "execution_count": 23, "id": "45165cca-5197-4590-ba69-7541085147f9", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/12/16 19:39:03 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n24/12/16 19:39:26 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"}], "source": "# Zapis wynik\u00f3w do tabeli \nfinal_df.write.mode(\"overwrite\").saveAsTable(df_result_table)"}, {"cell_type": "markdown", "id": "d0797752-450e-4f8f-a1d4-93a890a62c3d", "metadata": {}, "source": "Poni\u017cszy paragraf zapisuje metryki po uruchomieniu Twojego rozwi\u0105zania *misji g\u0142\u00f3wnej*. \n\nNie musisz go uruchamia\u0107 podczas implementacji rozwi\u0105zania."}, {"cell_type": "code", "execution_count": 24, "id": "c3647eae-2801-46ac-b43d-74e5bbfcab52", "metadata": {}, "outputs": [], "source": "# NIE ZMIENIA\u0106\nafter_df_metrics = get_current_metrics(spark_ui_address)"}, {"cell_type": "code", "execution_count": 25, "id": "8072527f-7151-4ec9-b86e-6d05686ac89e", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 39:===========================================>              (3 + 1) / 4]\r"}, {"name": "stdout", "output_type": "stream", "text": "+------------------+---------+-----------+-------------+------------+\n|       street_name|  borough|person_type|total_injured|total_killed|\n+------------------+---------+-----------+-------------+------------+\n|          2 AVENUE|MANHATTAN|    cyclist|          479|           0|\n|          BROADWAY|MANHATTAN|    cyclist|          468|           1|\n|          1 AVENUE|MANHATTAN|    cyclist|          416|           1|\n|   ATLANTIC AVENUE| BROOKLYN|   motorist|         2861|           5|\n|  LINDEN BOULEVARD| BROOKLYN|   motorist|         2409|           5|\n|NORTHERN BOULEVARD|   QUEENS|   motorist|         1935|           4|\n|          BROADWAY|MANHATTAN| pedestrian|         1050|           9|\n|          3 AVENUE|MANHATTAN| pedestrian|          858|          11|\n|          2 AVENUE|MANHATTAN| pedestrian|          846|           7|\n+------------------+---------+-----------+-------------+------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# final_df.show()\n\n# borough_above_average_df.show()"}, {"cell_type": "markdown", "id": "3bed01aa-cc23-427e-84c8-e5b76b9323bb", "metadata": {}, "source": "# Cz\u0119\u015b\u0107 3 - Pandas API on Spark\n\nTa cz\u0119\u015b\u0107 to wyzwanie. W szczeg\u00f3lno\u015bci dla os\u00f3b, kt\u00f3re nie programuj\u0105 na co dzie\u0144 w Pythonie, lub kt\u00f3re nie nie korzysta\u0142y do tej pory z Pandas API.  \n\nPowodzenia!\n\n## Misje poboczne\n\nW ponizszych paragrafach wprowad\u017a swoje rozwi\u0105zania *misji pobocznych*, o ile **nie** chcesz, aby oceniana by\u0142a *misja g\u0142\u00f3wna*. W przeciwnym przypadku **KONIECZNIE** pozostaw je **puste**.  "}, {"cell_type": "code", "execution_count": null, "id": "971a265f-db04-4a26-936d-18ab875ddffa", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "91621654-a24e-4ddb-b2c7-9f149252af13", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "9a5184ce-cf42-4342-aeec-b56c30b66bbd", "metadata": {}, "source": "## Misja g\u0142\u00f3wna \n\nPoni\u017cszy paragraf zapisuje metryki przed uruchomieniem Twojego rozwi\u0105zania *misji g\u0142\u00f3wnej*. \n\nNie musisz go uruchamia\u0107 podczas implementacji rozwi\u0105zania."}, {"cell_type": "code", "execution_count": 26, "id": "63fd8306-87e9-46f2-b622-d60693e3ba6d", "metadata": {"tags": []}, "outputs": [], "source": "#NIE ZMIENIA\u0106\nbefore_ps_metrics = get_current_metrics(spark_ui_address)"}, {"cell_type": "markdown", "id": "a967f079-7106-4bd7-9d26-98ced2aeb43b", "metadata": {}, "source": "W poni\u017cszych paragrafach wprowad\u017a **rozwi\u0105zanie** swojego projektu oparte o *Pandas API on Spark*. \n\nPami\u0119taj o wydajno\u015bci Twojego przetwarzania, *Pandas API on Spark* nie jest w stanie wszystkiego \"naprawi\u0107\". \n\nNie wprowadzaj w poni\u017cszych paragrafach \u017cadnego kodu, w przypadku wykorzystania *misji pobocznych*."}, {"cell_type": "code", "execution_count": 27, "id": "e45f80c8-4b66-41cd-8376-79bf82d27d61", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "/usr/lib/spark/python/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n  warnings.warn(\n/usr/lib/spark/python/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"}], "source": "import pyspark.pandas as ps\n\naccidents_ps = ps.read_csv(datasource1_dir, header=None)\nzip_borough_ps = ps.read_csv(datasource4_dir, header=0)\n\naccidents_ps.columns = [\n    \"crash_date\", \"_0\", \"zip_code\", \"_12\", \"_13\", \"_14\",\n    \"on_street_name\", \"cross_street_name\", \"off_street_name\", \"number_of_persons_injured\",\n    \"number_of_persons_killed\", \"number_of_pedestrians_injured\", \"number_of_pedestrians_killed\",\n    \"number_of_cyclist_injured\", \"number_of_cyclist_killed\", \"number_of_motorist_injured\",\n    \"number_of_motorist_killed\", \"_1\", \"_2\", \"_3\", \"_4\", \"_5\", \"_6\", \"_7\", \"_8\", \"_9\", \"_10\", \"_11\"\n]\n\nzip_borough_ps.columns = [col.lower().replace(\" \", \"_\") for col in zip_borough_ps.columns]"}, {"cell_type": "code", "execution_count": 28, "id": "67af510f-3fbb-4ad0-b5e9-62c125626df8", "metadata": {}, "outputs": [], "source": "accidents_ps[\"crash_date\"] = ps.to_datetime(accidents_ps[\"crash_date\"], format=\"%m/%d/%Y\", errors=\"coerce\")\n\nfiltered_accidents_ps = accidents_ps[\n    (accidents_ps[\"zip_code\"].notnull()) &\n    (accidents_ps[\"crash_date\"].notnull()) &\n    (accidents_ps[\"crash_date\"].dt.year > 2012)\n]\n\nfiltered_accidents_ps = filtered_accidents_ps[[\n    \"zip_code\", \"crash_date\", \"on_street_name\", \"cross_street_name\", \"off_street_name\",\n    \"number_of_pedestrians_injured\", \"number_of_pedestrians_killed\",\n    \"number_of_cyclist_injured\", \"number_of_cyclist_killed\",\n    \"number_of_motorist_injured\", \"number_of_motorist_killed\"\n]]\n\naccidents_with_borough_ps = filtered_accidents_ps.merge(zip_borough_ps, on=\"zip_code\", how=\"inner\")"}, {"cell_type": "code", "execution_count": 29, "id": "1b3ce182-f975-4b61-8e9d-08727615d22d", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/12/16 19:40:15 WARN AttachDistributedSequenceExec: clean up cached RDD(138) in AttachDistributedSequenceExec(2358)\n/usr/lib/spark/python/pyspark/pandas/groupby.py:893: FutureWarning: Default value of `numeric_only` will be changed to `False` instead of `True` in 4.0.0.\n  warnings.warn(\n"}], "source": "borough_data_ps = accidents_with_borough_ps.groupby(\"borough\").size().reset_index(name=\"total_count\")\naverage_count = borough_data_ps[\"total_count\"].mean()\nborough_above_average_ps = borough_data_ps[borough_data_ps[\"total_count\"] > average_count]\n#print(borough_above_average_ps)\nfiltered_accidents_with_borough_ps = accidents_with_borough_ps.merge(borough_above_average_ps[[\"borough\"]], on=\"borough\", how=\"inner\")\n\nstreet_data_ps = (\n    ps.concat(\n        [\n            filtered_accidents_with_borough_ps.assign(\n                street_type=\"on_street_name\",\n                street_name=filtered_accidents_with_borough_ps[\"on_street_name\"]\n            ),\n            filtered_accidents_with_borough_ps.assign(\n                street_type=\"cross_street_name\",\n                street_name=filtered_accidents_with_borough_ps[\"cross_street_name\"]\n            ),\n            filtered_accidents_with_borough_ps.assign(\n                street_type=\"off_street_name\",\n                street_name=filtered_accidents_with_borough_ps[\"off_street_name\"]\n            )\n        ]\n    )\n    .dropna(subset=[\"street_name\"])\n)\n\ndef filter_and_transform(psdf, injury_col, killed_col, person_type):\n    result = psdf[\n        (psdf[injury_col] > 0) | (psdf[killed_col] > 0)\n    ][[\n        \"street_name\", \"borough\", injury_col, killed_col\n    ]].rename(columns={\n        injury_col: \"injured\",\n        killed_col: \"killed\"\n    })\n    result[\"person_type\"] = person_type\n    return result\n\nperson_types = [\n    (\"number_of_pedestrians_injured\", \"number_of_pedestrians_killed\", \"pedestrian\"),\n    (\"number_of_cyclist_injured\", \"number_of_cyclist_killed\", \"cyclist\"),\n    (\"number_of_motorist_injured\", \"number_of_motorist_killed\", \"motorist\")\n]\n\n\nall_streets_ps = ps.concat([\n    filter_and_transform(street_data_ps, injury_col, killed_col, person_type)\n    for injury_col, killed_col, person_type in person_types\n])\n\n\ngrouped_streets_ps = (\n    all_streets_ps.groupby([\"street_name\", \"borough\", \"person_type\"])[[\"injured\", \"killed\"]]\n    .sum()\n    .rename(columns={\n        \"injured\": \"total_injured\",\n        \"killed\": \"total_killed\"\n    })\n    .reset_index()\n)"}, {"cell_type": "code", "execution_count": 30, "id": "570b4f1b-1aee-4535-94ad-38fc160bd136", "metadata": {}, "outputs": [], "source": "final_ps = \\\n    grouped_streets_ps \\\n    .sort_values(by=[\"person_type\", \"total_injured\", \"total_killed\"], ascending=[True, False, False]) \\\n    .groupby(\"person_type\").head(3) [[\"street_name\", \"borough\", \"person_type\", \"total_injured\", \"total_killed\"]]"}, {"cell_type": "code", "execution_count": 31, "id": "76e0d7f7-82f3-41d4-8267-cf288f2f6e81", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "/usr/lib/spark/python/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n24/12/16 19:40:36 WARN AttachDistributedSequenceExec: clean up cached RDD(159) in AttachDistributedSequenceExec(5918)\n24/12/16 19:40:43 WARN AttachDistributedSequenceExec: clean up cached RDD(170) in AttachDistributedSequenceExec(5964)\n24/12/16 19:40:55 WARN AttachDistributedSequenceExec: clean up cached RDD(181) in AttachDistributedSequenceExec(6018)\n24/12/16 19:41:00 WARN AttachDistributedSequenceExec: clean up cached RDD(192) in AttachDistributedSequenceExec(6120)\n24/12/16 19:41:12 WARN AttachDistributedSequenceExec: clean up cached RDD(203) in AttachDistributedSequenceExec(6222)\n24/12/16 19:41:21 WARN AttachDistributedSequenceExec: clean up cached RDD(214) in AttachDistributedSequenceExec(6324)\n24/12/16 19:41:28 WARN AttachDistributedSequenceExec: clean up cached RDD(225) in AttachDistributedSequenceExec(6426)\n24/12/16 19:41:34 WARN AttachDistributedSequenceExec: clean up cached RDD(236) in AttachDistributedSequenceExec(6528)\n24/12/16 19:41:42 WARN AttachDistributedSequenceExec: clean up cached RDD(247) in AttachDistributedSequenceExec(6630)\n24/12/16 19:41:48 WARN AttachDistributedSequenceExec: clean up cached RDD(258) in AttachDistributedSequenceExec(6732)\n24/12/16 19:41:53 WARN AttachDistributedSequenceExec: clean up cached RDD(292) in AttachDistributedSequenceExec(8369)\n                                                                                \r"}], "source": "final_ps.to_pandas().to_json(ps_result_file, orient='records')"}, {"cell_type": "markdown", "id": "298a0ec5-ab13-4e39-a572-e7adf8b8556a", "metadata": {}, "source": "Poni\u017cszy paragraf zapisuje metryki po uruchomieniu Twojego rozwi\u0105zania *misji g\u0142\u00f3wnej*. \n\nNie musisz go uruchamia\u0107 podczas implementacji rozwi\u0105zania."}, {"cell_type": "code", "execution_count": 32, "id": "108bee2a-a847-4625-8e4a-939951ac9201", "metadata": {}, "outputs": [], "source": "#NIE ZMIENIA\u0106\nafter_ps_metrics = get_current_metrics(spark_ui_address)"}, {"cell_type": "code", "execution_count": 33, "id": "b40b0b79-f5a1-4b48-986b-70fc7ed85a2f", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/12/16 19:42:12 WARN AttachDistributedSequenceExec: clean up cached RDD(317) in AttachDistributedSequenceExec(11988)\n24/12/16 19:42:19 WARN AttachDistributedSequenceExec: clean up cached RDD(328) in AttachDistributedSequenceExec(12034)\n24/12/16 19:42:31 WARN AttachDistributedSequenceExec: clean up cached RDD(339) in AttachDistributedSequenceExec(12088)\n24/12/16 19:42:37 WARN AttachDistributedSequenceExec: clean up cached RDD(350) in AttachDistributedSequenceExec(12190)\n24/12/16 19:42:48 WARN AttachDistributedSequenceExec: clean up cached RDD(361) in AttachDistributedSequenceExec(12292)\n24/12/16 19:42:56 WARN AttachDistributedSequenceExec: clean up cached RDD(372) in AttachDistributedSequenceExec(12394)\n24/12/16 19:43:05 WARN AttachDistributedSequenceExec: clean up cached RDD(383) in AttachDistributedSequenceExec(12496)\n24/12/16 19:43:13 WARN AttachDistributedSequenceExec: clean up cached RDD(394) in AttachDistributedSequenceExec(12598)\n24/12/16 19:43:19 WARN AttachDistributedSequenceExec: clean up cached RDD(405) in AttachDistributedSequenceExec(12700)\n24/12/16 19:43:26 WARN AttachDistributedSequenceExec: clean up cached RDD(416) in AttachDistributedSequenceExec(12802)\n24/12/16 19:43:29 WARN AttachDistributedSequenceExec: clean up cached RDD(450) in AttachDistributedSequenceExec(14460)\n"}, {"name": "stdout", "output_type": "stream", "text": "              street_name    borough person_type  total_injured  total_killed\n18179            2 AVENUE  MANHATTAN     cyclist            479             0\n18094            BROADWAY  MANHATTAN     cyclist            468             1\n19067            1 AVENUE  MANHATTAN     cyclist            416             1\n17555     ATLANTIC AVENUE   BROOKLYN    motorist           2861             5\n17687    LINDEN BOULEVARD   BROOKLYN    motorist           2409             5\n12289  NORTHERN BOULEVARD     QUEENS    motorist           1935             4\n9324             BROADWAY  MANHATTAN  pedestrian           1050             9\n8472             3 AVENUE  MANHATTAN  pedestrian            858            11\n14992            2 AVENUE  MANHATTAN  pedestrian            846             7\n"}], "source": "# print(final_ps)"}, {"cell_type": "markdown", "id": "e32e266b-b5cd-41d0-aeab-c1edc365910d", "metadata": {}, "source": "# Analiza wynik\u00f3w i wydajno\u015bci *misji g\u0142\u00f3wnych*"}, {"cell_type": "markdown", "id": "46b67111-62d0-4657-b158-1ed37db9ed96", "metadata": {}, "source": "## Cz\u0119\u015b\u0107 1 - Spark Core (RDD)"}, {"cell_type": "code", "execution_count": 34, "id": "5cfc9900-7e0c-49ff-adba-e339f83ffe51", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 241:=====================================================> (73 + 2) / 75]\r"}, {"name": "stdout", "output_type": "stream", "text": "('ATLANTIC AVENUE', 'BROOKLYN', 'motorist', 2861.0, 5.0)\n('LINDEN BOULEVARD', 'BROOKLYN', 'motorist', 2409.0, 5.0)\n('NORTHERN BOULEVARD', 'QUEENS', 'motorist', 1935.0, 4.0)\n('BROADWAY', 'MANHATTAN', 'pedestrian', 1050.0, 9.0)\n('3 AVENUE', 'MANHATTAN', 'pedestrian', 858.0, 11.0)\n('2 AVENUE', 'MANHATTAN', 'pedestrian', 846.0, 7.0)\n('2 AVENUE', 'MANHATTAN', 'cyclist', 479.0, 0.0)\n('BROADWAY', 'MANHATTAN', 'cyclist', 468.0, 1.0)\n('1 AVENUE', 'MANHATTAN', 'cyclist', 416.0, 1.0)\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Wczytanie wynik\u00f3w z pliku pickle\nword_counts = sc.pickleFile(rdd_result_dir)\n\n# Wy\u015bwietlenie 50 pierwszych element\u00f3w\nresult_sample = word_counts.take(50)\nfor item in result_sample:\n    print(item)"}, {"cell_type": "code", "execution_count": 35, "id": "16edae69-8062-4422-842f-d50bca0af9a7", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "{\n  \"numTasks\": 702,\n  \"numActiveTasks\": 0,\n  \"numCompleteTasks\": 602,\n  \"numFailedTasks\": 0,\n  \"numKilledTasks\": 0,\n  \"numCompletedIndices\": 602,\n  \"executorDeserializeTime\": 7679,\n  \"executorDeserializeCpuTime\": 3919912680,\n  \"executorRunTime\": 275684,\n  \"executorCpuTime\": 27507467732,\n  \"resultSize\": 1231620,\n  \"jvmGcTime\": 2384,\n  \"resultSerializationTime\": 436,\n  \"memoryBytesSpilled\": 0,\n  \"diskBytesSpilled\": 0,\n  \"peakExecutionMemory\": 0,\n  \"inputBytes\": 608888408,\n  \"inputRecords\": 3183830,\n  \"outputBytes\": 699,\n  \"outputRecords\": 3,\n  \"shuffleRemoteBlocksFetched\": 5652,\n  \"shuffleLocalBlocksFetched\": 5648,\n  \"shuffleFetchWaitTime\": 135,\n  \"shuffleRemoteBytesRead\": 2658212,\n  \"shuffleRemoteBytesReadToDisk\": 0,\n  \"shuffleLocalBytesRead\": 2648043,\n  \"shuffleReadBytes\": 5306255,\n  \"shuffleReadRecords\": 21298,\n  \"shuffleWriteBytes\": 5265655,\n  \"shuffleWriteTime\": 2816357989,\n  \"shuffleWriteRecords\": 20798\n}\n"}], "source": "subtract_metrics(after_rdd_metrics, before_rdd_metrics)"}, {"cell_type": "markdown", "id": "efc730f1-4b5e-4a68-8a86-11768918fcf4", "metadata": {}, "source": "## Cz\u0119\u015b\u0107 2 - Spark SQL (DataFrame)"}, {"cell_type": "code", "execution_count": 36, "id": "b950a09d-045e-4143-a3cf-8ecc7c73ac41", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 242:>                                                        (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+------------------+---------+-----------+-------------+------------+\n|       street_name|  borough|person_type|total_injured|total_killed|\n+------------------+---------+-----------+-------------+------------+\n|          2 AVENUE|MANHATTAN|    cyclist|          479|           0|\n|          BROADWAY|MANHATTAN|    cyclist|          468|           1|\n|          1 AVENUE|MANHATTAN|    cyclist|          416|           1|\n|   ATLANTIC AVENUE| BROOKLYN|   motorist|         2861|           5|\n|  LINDEN BOULEVARD| BROOKLYN|   motorist|         2409|           5|\n|NORTHERN BOULEVARD|   QUEENS|   motorist|         1935|           4|\n|          BROADWAY|MANHATTAN| pedestrian|         1050|           9|\n|          3 AVENUE|MANHATTAN| pedestrian|          858|          11|\n|          2 AVENUE|MANHATTAN| pedestrian|          846|           7|\n+------------------+---------+-----------+-------------+------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df = spark.table(df_result_table)\n\n# Wy\u015bwietlenie 50 pierwszych rekord\u00f3w\ndf.show(50)"}, {"cell_type": "code", "execution_count": 37, "id": "3f344ed9-94c1-4d79-b839-1839548d8c67", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "{\n  \"numTasks\": 172,\n  \"numActiveTasks\": 0,\n  \"numCompleteTasks\": 137,\n  \"numFailedTasks\": 0,\n  \"numKilledTasks\": 0,\n  \"numCompletedIndices\": 137,\n  \"executorDeserializeTime\": 8765,\n  \"executorDeserializeCpuTime\": 3999309410,\n  \"executorRunTime\": 152624,\n  \"executorCpuTime\": 65805695638,\n  \"resultSize\": 509018,\n  \"jvmGcTime\": 3698,\n  \"resultSerializationTime\": 223,\n  \"memoryBytesSpilled\": 0,\n  \"diskBytesSpilled\": 0,\n  \"peakExecutionMemory\": 2535397968,\n  \"inputBytes\": 1217845828,\n  \"inputRecords\": 4921011,\n  \"outputBytes\": 1960,\n  \"outputRecords\": 9,\n  \"shuffleRemoteBlocksFetched\": 20,\n  \"shuffleLocalBlocksFetched\": 20,\n  \"shuffleFetchWaitTime\": 0,\n  \"shuffleRemoteBytesRead\": 15283874,\n  \"shuffleRemoteBytesReadToDisk\": 0,\n  \"shuffleLocalBytesRead\": 13836565,\n  \"shuffleReadBytes\": 29120439,\n  \"shuffleReadRecords\": 1934741,\n  \"shuffleWriteBytes\": 29120439,\n  \"shuffleWriteTime\": 133353805,\n  \"shuffleWriteRecords\": 1934741\n}\n"}], "source": "subtract_metrics(after_df_metrics, before_df_metrics)"}, {"cell_type": "markdown", "id": "f063b46c-579d-4775-ba3f-837708279ea2", "metadata": {}, "source": "## Cz\u0119\u015b\u0107 3 - Pandas API on Spark"}, {"cell_type": "code", "execution_count": 38, "id": "ab5e31a2-fd31-40ca-be7b-b20b13dc38a2", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[\n  {\n    \"street_name\": \"2 AVENUE\",\n    \"borough\": \"MANHATTAN\",\n    \"person_type\": \"cyclist\",\n    \"total_injured\": 479,\n    \"total_killed\": 0\n  },\n  {\n    \"street_name\": \"BROADWAY\",\n    \"borough\": \"MANHATTAN\",\n    \"person_type\": \"cyclist\",\n    \"total_injured\": 468,\n    \"total_killed\": 1\n  },\n  {\n    \"street_name\": \"1 AVENUE\",\n    \"borough\": \"MANHATTAN\",\n    \"person_type\": \"cyclist\",\n    \"total_injured\": 416,\n    \"total_killed\": 1\n  },\n  {\n    \"street_name\": \"ATLANTIC AVENUE\",\n    \"borough\": \"BROOKLYN\",\n    \"person_type\": \"motorist\",\n    \"total_injured\": 2861,\n    \"total_killed\": 5\n  },\n  {\n    \"street_name\": \"LINDEN BOULEVARD\",\n    \"borough\": \"BROOKLYN\",\n    \"person_type\": \"motorist\",\n    \"total_injured\": 2409,\n    \"total_killed\": 5\n  },\n  {\n    \"street_name\": \"NORTHERN BOULEVARD\",\n    \"borough\": \"QUEENS\",\n    \"person_type\": \"motorist\",\n    \"total_injured\": 1935,\n    \"total_killed\": 4\n  },\n  {\n    \"street_name\": \"BROADWAY\",\n    \"borough\": \"MANHATTAN\",\n    \"person_type\": \"pedestrian\",\n    \"total_injured\": 1050,\n    \"total_killed\": 9\n  },\n  {\n    \"street_name\": \"3 AVENUE\",\n    \"borough\": \"MANHATTAN\",\n    \"person_type\": \"pedestrian\",\n    \"total_injured\": 858,\n    \"total_killed\": 11\n  },\n  {\n    \"street_name\": \"2 AVENUE\",\n    \"borough\": \"MANHATTAN\",\n    \"person_type\": \"pedestrian\",\n    \"total_injured\": 846,\n    \"total_killed\": 7\n  }\n]\n"}], "source": "import json\n\n# Odczytaj zawarto\u015b\u0107 pliku JSON\nwith open(ps_result_file, 'r') as file:\n    json_content = json.load(file)\n\n# Wy\u015bwietl zawarto\u015b\u0107\nprint(json.dumps(json_content, indent=2))"}, {"cell_type": "code", "execution_count": 39, "id": "32788c91-3f8e-4fb1-8afc-5eb00938e687", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "{\n  \"numTasks\": 644,\n  \"numActiveTasks\": 0,\n  \"numCompleteTasks\": 251,\n  \"numFailedTasks\": 0,\n  \"numKilledTasks\": 0,\n  \"numCompletedIndices\": 251,\n  \"executorDeserializeTime\": 6232,\n  \"executorDeserializeCpuTime\": 2342617797,\n  \"executorRunTime\": 405603,\n  \"executorCpuTime\": 147848510969,\n  \"resultSize\": 3262517,\n  \"jvmGcTime\": 5376,\n  \"resultSerializationTime\": 544,\n  \"memoryBytesSpilled\": 0,\n  \"diskBytesSpilled\": 0,\n  \"peakExecutionMemory\": 6599057792,\n  \"inputBytes\": 4017510950,\n  \"inputRecords\": 35780193,\n  \"outputBytes\": 0,\n  \"outputRecords\": 0,\n  \"shuffleRemoteBlocksFetched\": 48,\n  \"shuffleLocalBlocksFetched\": 40,\n  \"shuffleFetchWaitTime\": 0,\n  \"shuffleRemoteBytesRead\": 5826583,\n  \"shuffleRemoteBytesReadToDisk\": 0,\n  \"shuffleLocalBytesRead\": 3196734,\n  \"shuffleReadBytes\": 9023317,\n  \"shuffleReadRecords\": 448484,\n  \"shuffleWriteBytes\": 9023317,\n  \"shuffleWriteTime\": 160371449,\n  \"shuffleWriteRecords\": 448484\n}\n"}], "source": "subtract_metrics(after_ps_metrics, before_ps_metrics)"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}